"""Aggregate pose keypoint CSVs to identify the most active track in each clip.

This script scans a directory containing per-frame keypoint CSV files generated by
YOLO pose tracking (e.g., ``frames/pose_coords_yolo/<clip>/<frame>_coords.csv``).
For each clip it reconstructs track-wise trajectories, sums the per-frame
Euclidean displacement of all keypoints, and reports the track ID with the
largest total movement.

Usage example:
    python find_most_active_tracks.py --coords-root ../frames/pose_coords_yolo

If ``--output`` is supplied, a combined CSV summarising the movement metrics for
all clips is produced.
"""
from __future__ import annotations

import argparse
import math
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
import pandas as pd


@dataclass
class TrackStats:
    track_id: int
    total_movement: float = 0.0
    frames_seen: int = 0
    first_frame: Optional[str] = None
    last_frame: Optional[str] = None
    last_keypoints: Optional[np.ndarray] = field(default=None, repr=False)

    def update(self, frame_name: str, keypoints: np.ndarray) -> None:
        """Update the cumulative stats with the keypoints from a new frame."""
        if self.last_keypoints is not None and self.last_keypoints.shape == keypoints.shape:
            displacement = _keypoint_displacement(self.last_keypoints, keypoints)
            self.total_movement += displacement

        self.last_keypoints = keypoints
        self.frames_seen += 1
        if self.first_frame is None:
            self.first_frame = frame_name
        self.last_frame = frame_name


def _keypoint_displacement(prev: np.ndarray, curr: np.ndarray) -> float:
    """Compute summed Euclidean displacement between two keypoint sets."""
    if prev.size == 0 or curr.size == 0:
        return 0.0

    mask = np.isfinite(prev).all(axis=1) & np.isfinite(curr).all(axis=1)
    if not np.any(mask):
        return 0.0

    deltas = curr[mask] - prev[mask]
    distances = np.linalg.norm(deltas, axis=1)
    return float(distances.sum())


def _load_frame_tracks(frame_df: pd.DataFrame) -> Iterable[Tuple[int, str, np.ndarray]]:
    """Yield (track_id, frame_name, keypoints) tuples for a single frame."""
    if frame_df.empty:
        return []

    keypoint_columns = [col for col in frame_df.columns if col.startswith("kpt_")]
    if not keypoint_columns:
        return []

    keypoint_columns.sort()
    expected_pairs = len(keypoint_columns) // 2
    frame_name = str(frame_df["frame_name"].iloc[0]) if "frame_name" in frame_df.columns else ""

    records = []
    for _, row in frame_df.iterrows():
        track_id = int(row["track_id"]) if "track_id" in row else 0
        coords = row[keypoint_columns].to_numpy(dtype=float)
        keypoints = coords.reshape(expected_pairs, 2)
        records.append((track_id, frame_name, keypoints))
    return records


def _infer_frame_info(csv_path: Path, frame_df: pd.DataFrame) -> Tuple[int, str]:
    """Return (frame_index, frame_name) for ordering and reporting."""
    if not frame_df.empty:
        frame_index = int(frame_df["frame_index"].iloc[0]) if "frame_index" in frame_df.columns else math.inf
        frame_name = str(frame_df["frame_name"].iloc[0]) if "frame_name" in frame_df.columns else csv_path.stem
    else:
        frame_index = math.inf
        frame_name = f"{csv_path.stem.replace('_coords', '')}.jpg"
    return frame_index, frame_name


def process_clip(clip_dir: Path) -> List[TrackStats]:
    """Aggregate movement metrics for every track in a clip directory."""
    track_stats: Dict[int, TrackStats] = {}

    frame_files = sorted(clip_dir.glob("*_coords.csv"))
    frame_entries: List[Tuple[int, Path, pd.DataFrame, str]] = []

    for csv_path in frame_files:
        try:
            frame_df = pd.read_csv(csv_path)
        except Exception as exc:  # pragma: no cover - defensive
            print(f"‚ö†Ô∏è Failed to read {csv_path}: {exc}")
            continue
        frame_index, frame_name = _infer_frame_info(csv_path, frame_df)
        frame_entries.append((frame_index, csv_path, frame_df, frame_name))

    frame_entries.sort(key=lambda entry: (entry[0], entry[1].name))

    for _, csv_path, frame_df, _ in frame_entries:
        for track_id, frame_name, keypoints in _load_frame_tracks(frame_df):
            stats = track_stats.setdefault(track_id, TrackStats(track_id=track_id))
            stats.update(frame_name=frame_name, keypoints=keypoints)

    return list(track_stats.values())


def _discover_clip_dirs(coords_root: Path) -> List[Path]:
    """Return clip directories, supporting nested player/clip structures."""

    clip_dirs = {csv.parent for csv in coords_root.rglob("*_coords.csv")}
    if not clip_dirs:
        # Fall back to direct sub-directories to keep behaviour predictable when
        # CSVs have not been generated yet.
        clip_dirs = {p for p in coords_root.iterdir() if p.is_dir()}
    return sorted(clip_dirs)


def _normalize_relative_parts(parts: List[str]) -> List[str]:
    """Normalize relative path parts by removing any redundant segments.

    - Remove all occurrences of 'Cleaned_Data' anywhere in the path
    - Collapse duplicated player segments such as 'players/<P>/Cleaned_Data/<P>' ‚Üí 'players/<P>'
    """
    # Remove all 'Cleaned_Data'
    parts = [p for p in parts if p != "Cleaned_Data"]

    # Collapse duplicate player occurrences after 'players'
    if len(parts) >= 3 and parts[0] == "players":
        player = parts[1]
        # Remove immediate duplicated player tokens appearing again
        collapsed = [parts[0], player]
        i = 2
        while i < len(parts):
            if parts[i] == player and (i == 2 or collapsed[-1] == player):
                i += 1
                continue
            collapsed.append(parts[i])
            i += 1
        parts = collapsed

    return parts


def _clip_label(coords_root: Path, clip_dir: Path) -> str:
    """Create a display label relative to ``coords_root`` for logging/output."""

    relative = clip_dir.relative_to(coords_root)
    parts = _normalize_relative_parts(list(relative.parts))
    relative = Path(*parts) if parts else Path(".")
    if relative == Path("."):
        return clip_dir.name
    return relative.as_posix()


def summarise_clips(
    coords_root: Path,
    *,
    filter_to_most_active: bool = True,
    drop_keypoints: Optional[Sequence[int]] = None,
) -> List[Dict[str, object]]:
    """Process clips under the given root and return summary rows.

    If ``filter_to_most_active`` is True, each per-frame CSV is rewritten so that
    it only retains rows for the most active track.
    """
    summaries: List[Dict[str, object]] = []

    clip_dirs = _discover_clip_dirs(coords_root)
    if not clip_dirs:
        print(f"‚ö†Ô∏è No clip directories found under {coords_root}")
        return summaries

    for clip_dir in clip_dirs:
        clip_stats = process_clip(clip_dir)
        if not clip_stats:
            print(f"‚ö†Ô∏è {_clip_label(coords_root, clip_dir)}: no tracks detected")
            continue

        most_active = max(clip_stats, key=lambda s: s.total_movement)
        print(
            f"üèÉ {_clip_label(coords_root, clip_dir)}: track {most_active.track_id} moved the most "
            f"({most_active.total_movement:.2f} px across {most_active.frames_seen} frames)"
        )

        if filter_to_most_active or drop_keypoints:
            total_files, modified_files = rewrite_clip_frames(
                clip_dir,
                target_track_id=most_active.track_id if filter_to_most_active else None,
                drop_keypoints=drop_keypoints,
            )

            _rewrite_pose_tracks(
                coords_root,
                clip_dir,
                target_track_id=most_active.track_id if filter_to_most_active else None,
                drop_keypoints=drop_keypoints,
            )

            if modified_files:
                actions = []
                if filter_to_most_active:
                    actions.append(f"kept only track {most_active.track_id}")
                if drop_keypoints:
                    dropped = ", ".join(str(idx) for idx in drop_keypoints)
                    actions.append(f"dropped keypoints [{dropped}]")
                action_text = " and ".join(actions) if actions else "updated"
                clip_name = _clip_label(coords_root, clip_dir)
                print(f"‚úÇÔ∏è {clip_name}: {action_text} in {modified_files}/{total_files} frame CSVs")

        for stats in clip_stats:
            summaries.append(
                {
                    "clip": _clip_label(coords_root, clip_dir),
                    "track_id": stats.track_id,
                    "total_movement": stats.total_movement,
                    "frames_seen": stats.frames_seen,
                    "first_frame": stats.first_frame,
                    "last_frame": stats.last_frame,
                }
            )

    return summaries


def rewrite_clip_frames(
    clip_dir: Path,
    *,
    target_track_id: Optional[int] = None,
    drop_keypoints: Optional[Sequence[int]] = None,
) -> Tuple[int, int]:
    """Rewrite per-frame CSVs applying optional track filtering and column drops.

    Returns a tuple of (total_files, modified_files).
    """

    total_files = 0
    modified_files = 0

    drop_cols: List[str] = []
    if drop_keypoints:
        for idx in drop_keypoints:
            drop_cols.extend([f"kpt_{idx}_x", f"kpt_{idx}_y"])

    for csv_path in sorted(clip_dir.glob("*_coords.csv")):
        total_files += 1
        try:
            frame_df = pd.read_csv(csv_path)
        except Exception as exc:  # pragma: no cover - defensive
            print(f"‚ö†Ô∏è Failed to read {csv_path}: {exc}")
            continue

        new_df = frame_df
        modified = False

        if target_track_id is not None and "track_id" in new_df.columns:
            mask = new_df["track_id"] == target_track_id
            if mask.any() and not mask.all():
                new_df = new_df[mask].copy()
                modified = True
            elif not mask.any():
                new_df = new_df.iloc[0:0].copy()
                modified = True

        if drop_cols:
            existing = [col for col in drop_cols if col in new_df.columns]
            if existing:
                new_df = new_df.drop(columns=existing)
                modified = True

        if modified:
            new_df.to_csv(csv_path, index=False)
            modified_files += 1

    return total_files, modified_files


def _rewrite_pose_tracks(
    coords_root: Path,
    clip_dir: Path,
    *,
    target_track_id: Optional[int],
    drop_keypoints: Optional[Sequence[int]],
) -> None:
    """Apply filtering to aggregated pose_tracks artefacts for consistency."""

    if target_track_id is None and not drop_keypoints:
        return

    # pose_tracks „ÅØ frames „Å®ÂêåÈöéÂ±§
    tracks_root = coords_root.parent.parent / "pose_tracks"
    if not tracks_root.exists():
        return

    try:
        relative = clip_dir.relative_to(coords_root)
    except ValueError:
        relative = Path(clip_dir.name)
    parts = _normalize_relative_parts(list(relative.parts))
    relative = Path(*parts) if parts else Path("")

    track_dir = tracks_root / relative
    if not track_dir.exists():
        return

    drop_cols: List[str] = []
    if drop_keypoints:
        for idx in drop_keypoints:
            drop_cols.extend([f"kpt_{idx}_x", f"kpt_{idx}_y"])

    keypoints_path = track_dir / "keypoints_with_tracks.csv"
    if keypoints_path.exists():
        df = pd.read_csv(keypoints_path)
        modified = False

        if target_track_id is not None and "track_id" in df.columns:
            mask = df["track_id"] == target_track_id
            if mask.any() and not mask.all():
                df = df[mask].copy()
                modified = True
            elif not mask.any():
                df = df.iloc[0:0].copy()
                modified = True

        if drop_cols:
            keep_cols = [col for col in df.columns if col not in drop_cols]
            if len(keep_cols) != len(df.columns):
                df = df[keep_cols]
                modified = True

        if modified:
            df.to_csv(keypoints_path, index=False)

    summary_path = track_dir / "movement_summary.csv"
    if summary_path.exists() and target_track_id is not None:
        summary_df = pd.read_csv(summary_path)
        if "track_id" in summary_df.columns:
            filtered = summary_df[summary_df["track_id"] == target_track_id].copy()
            if not filtered.equals(summary_df):
                filtered.to_csv(summary_path, index=False)

def _resolve_coords_root(path_arg: Optional[Path]) -> Path:
    """Resolve the coords root relative to the script location if needed."""
    script_dir = Path(__file__).resolve().parent
    default_rel = Path("../frames/pose_coords_yolo")

    if path_arg is None:
        candidate = script_dir / default_rel
    else:
        candidate = path_arg.expanduser()
        if not candidate.is_absolute():
            candidate = script_dir / candidate

    return candidate.resolve()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Identify the most active track from pose keypoint CSVs")
    parser.add_argument(
        "--coords-root",
        type=Path,
        default=None,
        help="Directory containing per-frame keypoint CSVs",
    )
    parser.add_argument(
        "--keep-all-tracks",
        action="store_true",
        help="Skip rewriting CSVs so that all detected tracks remain",
    )
    parser.add_argument(
        "--drop-keypoints",
        type=int,
        nargs="*",
        help="Keypoint indices to remove (e.g. 0 1 2 3 4)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="Optional path to save the aggregated movement summary CSV",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    coords_root = _resolve_coords_root(args.coords_root)

    if not coords_root.exists():
        raise SystemExit(f"‚ùå coords-root does not exist: {coords_root}")

    summaries = summarise_clips(
        coords_root,
        filter_to_most_active=not args.keep_all_tracks,
        drop_keypoints=args.drop_keypoints,
    )

    if args.output and summaries:
        output_path = args.output.expanduser().resolve()
        output_path.parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame(summaries).to_csv(output_path, index=False)
        print(f"üíæ Summary written to {output_path}")


if __name__ == "__main__":
    main()